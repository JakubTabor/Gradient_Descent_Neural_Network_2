# Gradient_Descent_Neural_Network_2
I gonna build neural network from scratch also with gradient descent inside 
# I gonna build some helper functions which i gonna use later in my model
# First I need activation function for my model so I define sigmoid function, It will eturn value in  range 0 to 1
# Now my "prediction_function", so i fill paterrn which is (w1 * x1 + w2 * x2 + bias) I save it as "weighted_sum" and return activation of my "weighted_sum"
# Then I check if my function works good and it seems that everything is alright
# Next I build "log loss" function, in simple words it take values in range 0 to 1, very close to this numbers but not exact numbers
# Then it return mean log loss, so i use "-np.mean" to get mean value and "np.log" to get log of y_predicted_new and log of "1 - y_predicted_new"
# And "y_predicted_new" is first my value close to 0 "[max(i, epsilon) for i in y_predicted]" and the value close to 1 "[min(i, 1 - epsilon) for i in y_predicted_new]"
# I change my activation function into numpy, I look at values that function returns, they are in range 0 to 1
# Now I can build Neural Network, so I start with constructor function, define my weights and bias  
# Next i define fit function, it will train my model, so I call gradient descent function, that I will build in a moment
# Inside it I put "age of X and affordibility of X", then y, then epochs and loss_thresold, all will be return in form of tuple
# loss_thresold define where my function need to stop to, I take the number from my keras model 
# I save it as my weights and bias, I pass this arguments for my gradient descent and it will calculate the output values
# And It will print my weights and bias for comparison to my keras model 
# Then I define predict function, so w1 * x1 + w2 * x2 + bias and in that case i take x1 and x2 from my X_test 
# I save it as weighted_sum and put variable into activation function, so previous defined "sigmoid_numpy" 
# Now its time to define gradient descent, first I initialize my weights with ones and bias with zero
# Start values of "learning_rate" will be 0.5, my columns have the same length so it doesn't matter from which i take length for my variable n
# Then I start for loop which range is number of my epochs, that I define later at final stage 
# Now I put inside weighted sum which I use also in predict function, i define it to get y_predicted 
# Next y_predicted which is weighted_sum in activation function, I gonna use difference between y_predicted and y_true in several calculation
# Then I define loos which is my log loss function of (y_predicted and y_true), so it gonna calculate log loss on this two 
# Now I can calculate derivatives, so first will be w1 derivative
# I need to transpose age, then (sum up "age" and difference between y_predicted - y_true) using np.dot, so i take dot product
# And take average of it using (1 / n) and "n" is length of my age column, thet was w1 derivative
# Then I calculate w2 derivative, so i do this same calculations but for affordability
