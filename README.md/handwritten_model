

# Gradient_Descent_Neural_Network_2
I gonna build neural network from scratch also with gradient descent inside 

# I gonna build some helper functions which i gonna use later in my model

# First I need activation function for my model so I define sigmoid function, It will return value in  range 0 to 1

# Now my "prediction_function", so i fill paterrn which is (w1 * x1 + w2 * x2 + bias) I save it as "weighted_sum" and return activation of my "weighted_sum"
# Then I check if my function works good and it seems that everything is alright

# Next I build "log loss" function, in simple words it take values in range 0 to 1, very close to this numbers but not exact numbers
# Then it return mean log loss, so i use "-np.mean" to get mean value and "np.log" to get log of y_predicted_new and log of "1 - y_predicted_new"
# And "y_predicted_new" is first my value close to 0 "[max(i, epsilon) for i in y_predicted]" and the value close to 1 "[min(i, 1 - epsilon) for i in y_predicted_new]"
# Loss decreases as the performance gets better

# I change my activation function into numpy, I look at values that function returns, they are in range 0 to 1

# Now I can build Neural Network, so I start with constructor function, define my weights and bias  
# Next i define fit function, it will train my model, so I call gradient descent function, that I will build in a moment
# Inside it I put "age of X and affordibility of X", then y, then epochs and loss_thresold, all will be return in form of tuple

# loss_thresold define where my function need to stop to, I take the number from my keras model 
# I save it as my weights and bias, I pass this arguments for my gradient descent and it will calculate the output values
# And It will print my weights and bias for comparison to my keras model 

# Then I define predict function, so w1 * x1 + w2 * x2 + bias and in that case i take x1 and x2 from my X_test 
# I save it as weighted_sum and put variable into activation function, so previous defined "sigmoid_numpy" 

# Now its time to define gradient descent, first I initialize my weights with ones and bias with zero
# Start values of "learning_rate" will be 0.5, my columns have the same length so it doesn't matter from which i take length for my variable n

# Then I start for loop which range is number of my epochs, that I define later at final stage 
# Now I put inside weighted sum which I use also in predict function, i define it to get y_predicted 

# Next y_predicted which is weighted_sum in activation function, I gonna use difference between y_predicted and y_true in several calculation

# Then I define loos which is my log loss function of (y_predicted and y_true), so it gonna calculate log loss on this two 

# Now I can calculate derivatives, so first will be w1 derivative
# I need to transpose age, then (sum up "age" and difference between y_predicted - y_true) using np.dot, so i take dot product
# And take average of it using (1 / n) and "n" is length of my age column, thet was w1 derivative
# Then I calculate w2 derivative, so i do this same calculations but for affordability
# And bias derivative, so average of difference between y_predicted and y_true

# Now I can calculate w1, w2 and bias, I use previous defined learning rate and w1, w2 derivatives, also this same for bias 

# I set i to every fifty percent, it means that it gonna record every fifty epoch my progress and I print it
# Then I set condition (if my loss is equal loss_thresold, so loss from my previous model), I gonna put number of this loss into object of my function
# My function stop at this point and my weights and bias will be this same as from my keras model

# So I gonna pirint it also and brake will stop my fuction, I return weights and bias
# No I can create object of my class and call fit fuction from my model 
# I put inside X_train_scaled and y_train, set number of epochs at 500, and set loss_thresold 

# Now I can call coef and intercept which will return my weights and bias from previous keras model
# My weights and bais look pretty much the same, I also look at predictions and looks almost the same

# Results show that my models are very similar and also show how its easy to build neural network with keras
# With simple few lines of code i get network with good accuracy, in meantime i need whole class with a lot of code to get this same results
# And to not make a mistake in handwritten model, You need good understanding of code and math, with keras model big part of work is done for you
