# Mini Batch Gradient Descent use batch of randomly picked samples for a forward pass and then adjust weights
# In comparison to (Batch Gradient Descent) which use "all" training samples and (Stochastic Gradient Descent) which use "one" random sample for forward pass
# Its also good for big datasets, as well as "SGD", because it use batch of samples to train the model
# I import and prepare my dataset as previous and scale all columns
# Now I can build "Mini Batch Gradient Descent", first i put some features, (X and y_true, number of epochs, batch size and learning rate)
# Then I define "number_of_features", which are (area and bedroom), it will be "X.shape[1]"
# Then I initialize weights with "ones" and pass shape which are my previous defined features
# And I also initialize bias with "zero", my columns have equal size, so I define "total_samples" as "X.shape[0]"
# Now I define batches for my "Gradien Descent", so I specify if "total samples" are less than "variable batch_size" which is (5)
# Then if this condition is true "batch_size will be equal total samples", this is a simple python trick, thats the way how it gonna pick batches
# And I create two lists to record epochs and cost
# Then I create for loop in range of my epochs and inside I define variable "random indices"
# "random.permutation" from numpy, it gonna take "all samples" and pick "random sample"
# And I gonna take random samples from "X and y_true", I save it as "X_tmp and y_tmp"
