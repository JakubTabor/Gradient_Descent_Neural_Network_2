# Now It's time to build stochastic gradient descent, I need to import random for it

# I gonna build the function that same as previous, so X, y_true, epochs and learning rate, which I set to (0.01)
# Now I gonna pass this same variables as previous, so I define "number_of_features", weights, bias and "total_samples"   
# Then I create two lists to record epochs and cost, to later plot them
# I start for loop in length of my epochs, and now I gonna use previous imported "random", to choose random samples
# I use "randint" to pick random samples between (zero and total_samples - 1) indexes in python are -1, I save it as "random_index "
# Now I need to pick samples from X and y_true, so I create two variables for it
# First will be (X of "random_index") and (y_true of "random_index"), I save it as "sample_x" and "sample_y" 
# Now I define "y_predicted", which is dot product between (w and transpose "sample_x") plus some bias 
# Then I calculate first derivatives, which is random sample of X transpose, dot product of difference between true sample and predicted
# And I take mean of it multiply by minus two, it will be variable "w_grad"
# Next my second derivatives for bias, it is the same, but in this case I take mean of y sample and predicted y, without dot product
# Now I can calculate first weights, some variable minus learning rate into w_grad variable, which I calculate above
# And I do this same with bias, now I have defined bias and weights
# Then I calculate cost, which is squared difference between sample of y and predicted y
# Now I specify that cost and epoch will be recorded every 100th iteration, they will append in my lists
# I return from my function weights, bias cost, cost_list, epoch_list
# Now I create object of my function "stochastic_gradient_descent", and put inside X_scaled, y_scaled and reshaped to "y_scaled.shape[0]", I pass (10000) epochs
# And I get weight, bias and cost of my "stochastic_gradient_descent", they are the same as form "batch_gradient_descent"
# But for "stochastic_gradient_descent" I need to set more epochs, because every epoch it pick only one sample
# Then I plot of graph previous saved in my lists (cost and epochs), we can se that its not that smoth as on previous plot
# But later after 4000 epoch it is more stable
# I also check some predictions and they look pretty realistic, when I put (this same number of square feets but with one more bedroom) the price is higher
# Conclusions are that (Batch Gradient Descent) have good performance on small datasets, he use all training samples in one forward pass and then adjust weights
# In comparison (Stochastic Gradient Descent) use only one sample in forward pass and after adjust weights
# It is good when you have big dataset, because it saves a lot of computation, When you have a lot of computation power you can use also (Stochastic Gradient Descent)
